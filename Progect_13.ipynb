{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBTpWTllpU5C"
      },
      "outputs": [],
      "source": [
        "\n",
        "Анализ поведения пользователей в мобильном приложении\n",
        "\n",
        "Цель исследования: выделить основные сценарии использования приложения и предложить гипотезы по улучшению юзер экспириенса\n",
        "\n",
        "Предобработка данных:\n",
        "\n",
        "загрузка и чтение датасетов;\n",
        "проверка типов данных;\n",
        "приведение названий столбцов к snake_case;\n",
        "проверка данных на наличие пропусков и дубликатов.\n",
        "Исследовательский анализ данных:\n",
        "изучение количества событий каждого типа;\n",
        "анализ распределения количества действий юзерами;\n",
        "анализ среднего количества действий на юзера по каналу привлечения;\n",
        "расчет метрик - DAU, WAU, retention rate на N-ый день - по ходу исследования набор метрик может корректироваться;\n",
        "изучение различий в паттернах поведения юзеров, использующих и игнорирующих рекомендации:\n",
        "анализ доли целевого трафика по каналу привлечения (целевой трафик - юзеры, использующие рекомендации);\n",
        "изучение количества просмотренных контактов на 1 юзера для каждого сегмента;\n",
        "анализ конверсии в целевое действие - просмотр контактов - по этим сегментам;\n",
        "Проведение стат. тестов:\n",
        "Конверсия в просмотры контактов у пользователей, совершающих действия tips_show и tips_click, отличается от тех, кто только смотрит рекомендации - только tips_show\n",
        "H0: конверсия в просмотр контактов у двух сегментов юзеров одинаковая;\n",
        "H1: конверсия в просмотр контактов у двух сегментов юзеров разная;\n",
        "выводы по полученной информации;\n",
        "2)Конверсия в просмотр контактов из поиска и рекомендаций одинаковая\n",
        "\n",
        "то есть мы объединяем все серчи в один блок - подразумевается, что это какие-либо фильтры, поиски по популярным названия / категориям товаров или кастомный поиск - получаем в результате теста метрику \"качества\" наших рекомендаций;\n",
        "таким образом, с помощью этой гипотезы мы сможем понять насколько точны наши рекомендации, то есть если конверсия в просмотр контактов по рекомендованным объявлениям гораздо ниже, то стоит сообщить об этом команде, занимающейся алгоритмами рекомендаций\n",
        "H0: конверсия в просмотр контактов из поиска и рекомендаций одинаковая;\n",
        "H1: конверсия в просмотр контактов из поиска и рекомендаций разная;\n",
        "выводы по полученной информации;\n",
        "Проработка основных вопросов анализа:\n",
        "Какие сценарии использования приложения выделяются?\n",
        "выделение наиболее популярных сценариев;\n",
        "предположения того, почему сложилась именно такие сценарии использования;\n",
        "выводы по полученной информации;\n",
        "Как различается время между событиями advert_open -> contacts_show и tips_click -> contacts_show. Какая конверсия в целевое действие у данных действий?\n",
        "расчет необходимых метрик;\n",
        "выводы по полученной информации;\n",
        "Выводы и рекомендации\n",
        "Для более быстрого погружения в ситуацию предоставляю описание данных:\n",
        "\n",
        "Датасет содержит данные о событиях, совершенных в мобильном приложении \"Ненужные вещи\". В нем пользователи продают свои ненужные вещи, размещая их на доске объявлений.\n",
        "\n",
        "В датасете содержатся данные пользователей, впервые совершивших действия в приложении после 7 октября 2019 года.\n",
        "\n",
        "Колонки в mobile_sources.csv:\n",
        "\n",
        "userId — идентификатор пользователя,\n",
        "source — источник, с которого пользователь установил приложение.\n",
        "Колонки в mobile_dataset.csv:\n",
        "\n",
        "event.time — время совершения,\n",
        "user.id — идентификатор пользователя,\n",
        "event.name — действие пользователя.\n",
        "Виды действий:\n",
        "\n",
        "advert_open — открыл карточки объявления,\n",
        "photos_show — просмотрел фотографий в объявлении,\n",
        "tips_show — увидел рекомендованные объявления,\n",
        "tips_click — кликнул по рекомендованному объявлению,\n",
        "contacts_show и show_contacts — посмотрел номер телефона,\n",
        "contacts_call — позвонил по номеру из объявления,\n",
        "map — открыл карту объявлений,\n",
        "search_1—search_7 — разные действия, связанные с поиском по сайту,\n",
        "favorites_add — добавил объявление в избранное.\n",
        "Шаг 2. Предобработка данных\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.subplots as sp\n",
        "import statistics as stat\n",
        "from scipy import stats as st\n",
        "import math as mth\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "\n",
        "# импортируем библиотеки\n",
        "# скроем будущие предупреждения об ошибках\n",
        "warnings.filterwarnings('ignore')\n",
        "mob_df = pd.read_csv('https://code.s3.yandex.net/datasets/mobile_dataset.csv')\n",
        "mob_sources = pd.read_csv('https://code.s3.yandex.net/datasets/mobile_sources.csv')\n",
        "# загружаем датасеты\n",
        "mob_df\n",
        "# смотрим на датасет с ифнормацией о действиях юзеров\n",
        "event.time\tevent.name\tuser.id\n",
        "0\t2019-10-07 00:00:00.431357\tadvert_open\t020292ab-89bc-4156-9acf-68bc2783f894\n",
        "1\t2019-10-07 00:00:01.236320\ttips_show\t020292ab-89bc-4156-9acf-68bc2783f894\n",
        "2\t2019-10-07 00:00:02.245341\ttips_show\tcf7eda61-9349-469f-ac27-e5b6f5ec475c\n",
        "3\t2019-10-07 00:00:07.039334\ttips_show\t020292ab-89bc-4156-9acf-68bc2783f894\n",
        "4\t2019-10-07 00:00:56.319813\tadvert_open\tcf7eda61-9349-469f-ac27-e5b6f5ec475c\n",
        "...\t...\t...\t...\n",
        "74192\t2019-11-03 23:53:29.534986\ttips_show\t28fccdf4-7b9e-42f5-bc73-439a265f20e9\n",
        "74193\t2019-11-03 23:54:00.407086\ttips_show\t28fccdf4-7b9e-42f5-bc73-439a265f20e9\n",
        "74194\t2019-11-03 23:56:57.041825\tsearch_1\t20850c8f-4135-4059-b13b-198d3ac59902\n",
        "74195\t2019-11-03 23:57:06.232189\ttips_show\t28fccdf4-7b9e-42f5-bc73-439a265f20e9\n",
        "74196\t2019-11-03 23:58:12.532487\ttips_show\t28fccdf4-7b9e-42f5-bc73-439a265f20e9\n",
        "74197 rows × 3 columns\n",
        "\n",
        "Необходимо привести названия столбцов к snake_case, а далее посмотрим общую информацию по датасету\n",
        "\n",
        "mob_df.columns = mob_df.columns.str.replace('.', '_')\n",
        "# приводим столбцы к snake_case\n",
        "mob_df.info()\n",
        "# смотрим общую инфу по дф\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 74197 entries, 0 to 74196\n",
        "Data columns (total 3 columns):\n",
        " #   Column      Non-Null Count  Dtype\n",
        "---  ------      --------------  -----\n",
        " 0   event_time  74197 non-null  object\n",
        " 1   event_name  74197 non-null  object\n",
        " 2   user_id     74197 non-null  object\n",
        "dtypes: object(3)\n",
        "memory usage: 1.7+ MB\n",
        "Тип данных у столбца с датой события неверный, необходимо его привести к datetime, а также поскольку мы имеем много знаков после запятой - округлим время до секунд, однако перед началом работы со столбцом 'event_time' стоит проверить, есть ли у нас дубликаты. Так как после округления времени, они могут появиться - но если до округления их не было, то стоит оставить данные дубли.\n",
        "\n",
        "mob_df.duplicated().sum()\n",
        "# количество явных дубликатов\n",
        "0\n",
        "mob_df['event_time'] = pd.to_datetime(mob_df['event_time'], format='%Y.%m.%d %H:%M:%S').dt.round('1S')\n",
        "# приводим столбец с о временем события в порядок\n",
        "mob_df.duplicated().sum()\n",
        "# количество явных дубликатов после округления времени события\n",
        "1118\n",
        "Итоги работы с датасетом с ифнормацией о действиях юзеров:\n",
        "\n",
        "пропусков и явных дубликатов нет;\n",
        "названия столбцов приведены к snake_case;\n",
        "тип данных столбца 'event_time' приведен к datetime, а также время события округлено до секунд;\n",
        "несмотря на то, что у нас появилось 1118 дубликатов, мы их не будем удалять, так как это произошло из-за округления времени события до секунд\n",
        "mob_sources\n",
        "# смотрим на датасет с ифнормацией об источниках трафика\n",
        "userId\tsource\n",
        "0\t020292ab-89bc-4156-9acf-68bc2783f894\tother\n",
        "1\tcf7eda61-9349-469f-ac27-e5b6f5ec475c\tyandex\n",
        "2\t8c356c42-3ba9-4cb6-80b8-3f868d0192c3\tyandex\n",
        "3\td9b06b47-0f36-419b-bbb0-3533e582a6cb\tother\n",
        "4\tf32e1e2a-3027-4693-b793-b7b3ff274439\tgoogle\n",
        "...\t...\t...\n",
        "4288\tb86fe56e-f2de-4f8a-b192-cd89a37ecd41\tyandex\n",
        "4289\t424c0ae1-3ea3-4f1e-a814-6bac73e48ab1\tyandex\n",
        "4290\t437a4cd4-9ba9-457f-8614-d142bc48fbeb\tyandex\n",
        "4291\tc10055f0-0b47-477a-869e-d391b31fdf8f\tyandex\n",
        "4292\td157bffc-264d-4464-8220-1cc0c42f43a9\tgoogle\n",
        "4293 rows × 2 columns\n",
        "\n",
        "mob_sources = mob_sources.rename(columns={'userId': 'user_id'})\n",
        "# приводим столбцы к snake_case\n",
        "mob_sources.info()\n",
        "# смотрим общую инфу по дф\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 4293 entries, 0 to 4292\n",
        "Data columns (total 2 columns):\n",
        " #   Column   Non-Null Count  Dtype\n",
        "---  ------   --------------  -----\n",
        " 0   user_id  4293 non-null   object\n",
        " 1   source   4293 non-null   object\n",
        "dtypes: object(2)\n",
        "memory usage: 67.2+ KB\n",
        "mob_sources.duplicated().sum()\n",
        "# количество явных дубликатов\n",
        "0\n",
        "Пропусков и дубликатов также нет, названия столбцов приведены к snake_case\n",
        "\n",
        "Итоги предобработки анализ данных:\n",
        "\n",
        "пропусков и явных дубликатов в первоначаьных данных нет;\n",
        "названия столбцов приведены к snake_case;\n",
        "для датасета mob_df тип данных столбца 'event_time' приведен к datetime, а также время события округлено до секунд;\n",
        "несмотря на то, что у нас появилось 1118 дубликатов, мы их не будем удалять, так как это произошло из-за округления времени события до секунд\n",
        "Шаг 3. Исследовательский анализ данных\n",
        "# объединим два дф в один для более удобной работы\n",
        "mob_df = mob_df.merge(mob_sources, how='left', on='user_id')\n",
        "# посмотрим сколько событий каждого типа содержится в нашем дф\n",
        "mob_df['event_name'].value_counts()\n",
        "tips_show        40055\n",
        "photos_show      10012\n",
        "advert_open       6164\n",
        "contacts_show     4450\n",
        "map               3881\n",
        "search_1          3506\n",
        "favorites_add     1417\n",
        "search_5          1049\n",
        "tips_click         814\n",
        "search_4           701\n",
        "contacts_call      541\n",
        "search_3           522\n",
        "search_6           460\n",
        "search_2           324\n",
        "search_7           222\n",
        "show_contacts       79\n",
        "Name: event_name, dtype: int64\n",
        "Поскольку мы не имеем точной информации о том, какое именно действие подразумевается под \"search_1\" - \"search_7\", то мы объединим их все в одну категорию \"search\".\n",
        "\n",
        "Также мы имеем 2 события очень похожих друг на друга - \"contacts_show\" и \"show_contacts\" - скорее всего, данные хранятся в двух БД, поэтому при выгрузке получилась подобная ситуация. Данные события мы объединим в категорию \"contacts_show\"\n",
        "\n",
        "# заменим значения в соответствии с планом выше\n",
        "mob_df['event_name'] = mob_df['event_name'].str.replace('search_+\\d', 'search', regex=True)\n",
        "mob_df.loc[mob_df['event_name'] == 'show_contacts', 'event_name'] = 'contacts_show'\n",
        "# проверим корректность наших преобразований\n",
        "mob_df['event_name'].value_counts()\n",
        "tips_show        40055\n",
        "photos_show      10012\n",
        "search            6784\n",
        "advert_open       6164\n",
        "contacts_show     4529\n",
        "map               3881\n",
        "favorites_add     1417\n",
        "tips_click         814\n",
        "contacts_call      541\n",
        "Name: event_name, dtype: int64\n",
        "# построим гистограмму распределения количества действий каждого пользователя\n",
        "mob_df.groupby('user_id')['event_time'].count().hist()\n",
        "\n",
        "plt.title('Гистограмма распределения количества действий каждого пользователя', loc='left')\n",
        "plt.xlabel('Количество действий на юзера')\n",
        "plt.ylabel('Количество юзеров')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "По гистограмме видно, что большинство пользователей совершает до 50 действий, а также в данных есть выбросы - юзеры, совершившие порядка 500 событий. Выбросы пока что мы оставим, возможно, удастся понять, почему они совершают столько действий.\n",
        "\n",
        "Далее мы изучим, сколько юзеров переходят по нашим рекомендациям, а сколько игнорируют их. Возможно, дело как раз таки в рекомендациях, и юзеры, использующих их, совершают в среднем гораздо больше действий.\n",
        "\n",
        "Далее мы посмотрим основную стат. информацию по количеству событий на юзера\n",
        "\n",
        "# выводим основную стат. информацию по столбцу с количеством действий каждого юзера\n",
        "mob_df.groupby('user_id')['event_time'].count().describe()\n",
        "count    4293.000000\n",
        "mean       17.283252\n",
        "std        29.130677\n",
        "min         1.000000\n",
        "25%         5.000000\n",
        "50%         9.000000\n",
        "75%        17.000000\n",
        "max       478.000000\n",
        "Name: event_time, dtype: float64\n",
        "В среднем наши юзеры совершают по 17 действий, медианное же значение - 9 действий. Из этого можно сделать следующие выводы:\n",
        "\n",
        "у нас есть большой \"хвост\" из юзеров, которые соверашают очень мало действий, но также 25% крайне активных юзеров - с количеством событий сильно больше среднего значения - 17 - которые и перекашивают наше распределение (25% потому что у нас есть информация о 3 квартиле (Q3), который равен 17 событиям);\n",
        "распределение имеет положительную ассиметрию - правосторонний скос;\n",
        "так же по максимальному значению - 478 действий - можно понять, что среди наших 25% активных пользователей есть сегмент пользователей, который даже на фоне этих 25% самых активных юзеров, будет сильно выделяться - скорее всего, это люди, занимающиеся перепродажей или магазины / мелкие предприниматели, которые на нашей площадке продают не только свои ненужные вещи;\n",
        "учитывя поинты выше, и факт того, что стандартное отклонение равно 29, при среднем 17 - это подтверждает, что у нас есть много пользователей с небольшой активностью 5 - 9 событий, а также немало юзеров с крайне высокой активностью - сильно больше 17;\n",
        "Далее мы перейдем к изучению поведения пользователей, которые используют наши рекомендации, и которые игнорируют их.\n",
        "\n",
        "# создадим дф с пользователями, использующими рекомендации\n",
        "tips_lovers =  mob_df[mob_df['user_id'].isin(mob_df[mob_df['event_name'] == 'tips_click']['user_id'])]\n",
        "# выведем кол-во таких юзеров\n",
        "tips_lovers['user_id'].nunique()\n",
        "322\n",
        "# создадим дф с пользователями, не использующими рекомендации\n",
        "tips_haters = mob_df[~mob_df['user_id'].isin(tips_lovers['user_id'])]\n",
        "# выведем кол-во таких юзеров\n",
        "tips_haters['user_id'].nunique()\n",
        "3971\n",
        "Таким образом, в нашем датасете содержится информация о 322 пользователях, которые переходят по нашим рекомендациям, и 3971 - не переходят.\n",
        "\n",
        "Анализ различий в поведении пользователей, использующих и игнорирующих рекомендации\n",
        "Далее мы узнаем, сколько в среднем просмотров контактов приходится на пользователя, и одинаково ли это значение для двух исследуемых категорий пользователей - использующих и игнорирующих рекомендации (реки)\n",
        "\n",
        "# посчитаем общее количество просмотренных контактов юзерами, использующими реки\n",
        "(tips_lovers[tips_lovers['event_name'].isin(['contacts_show'])]\n",
        "          .groupby('user_id', as_index=False)['event_name']\n",
        "          .count()['event_name'].sum())\n",
        "869\n",
        "# посчитаем количество просмотренных контактов на 1 юзера, использующих реки\n",
        "(tips_lovers[tips_lovers['event_name'].isin(['contacts_show'])]\n",
        "          .groupby('user_id', as_index=False)['event_name']\n",
        "          .count()['event_name'].sum() / tips_lovers['user_id'].nunique())\n",
        "2.698757763975155\n",
        "Таким образом, 322 пользователя, которые переходят по нашим рекомендациям, просмотрели 869 контактов продавцов или же почти 2,7 просмотра контакта на юзера\n",
        "\n",
        "# посчитаем общее количество просмотренных контактов юзерами, не использующих реки\n",
        "(tips_haters[tips_haters['event_name'].isin(['contacts_show'])]\n",
        "          .groupby('user_id', as_index=False)['event_name']\n",
        "          .count()['event_name'].sum())\n",
        "3660\n",
        "# посчитаем количество просмотренных контактов на 1 юзера, не использующего реки\n",
        "(tips_haters[tips_haters['event_name'].isin(['contacts_show'])]\n",
        "          .groupby('user_id', as_index=False)['event_name']\n",
        "          .count()['event_name'].sum() / tips_haters['user_id'].nunique())\n",
        "0.9216821959204231\n",
        "В случае с 3971 пользователями, которые игнорят по наши рекомендации, просмотров контактов продавцов оказолось лишь 3660 или же 0,92 просмотра контакта на юзера\n",
        "\n",
        "Далее мы проверим на статистическом тесте гипотезу о равнестве конверсии в просмтор контакта между 2 сегментами пользователей - использующих и игнорирующих наши рекомендации - и выясним совпадает ли конверсия в просмотр контактов\n",
        "\n",
        "# осортируем данные по айдишнику юзера и дате ивента\n",
        "# чтобы корректно разбить действия по сессиям\n",
        "mob_df = mob_df.sort_values(['user_id', 'event_time'])\n",
        "# зададим начальное значение и шаг для выбора интервала\n",
        "initial_interval = pd.Timedelta('5 minutes')\n",
        "increment = pd.Timedelta('10 minutes')\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in range(4):\n",
        "    # определяем интервал\n",
        "    interval = initial_interval + i * increment\n",
        "\n",
        "    # считаем количество сессий при заданном интервале\n",
        "    g = (mob_df.groupby('user_id')['event_time'].diff() > interval).cumsum()\n",
        "    mob_df['session_id'] = mob_df.groupby(['user_id', g], sort=False).ngroup() + 1\n",
        "    num_sessions = mob_df['session_id'].nunique()\n",
        "\n",
        "    # добавляем полученной значение в список\n",
        "    results.append([str(interval), num_sessions])\n",
        "\n",
        "# создаем дф из полученных данных\n",
        "df = pd.DataFrame(results, columns=['interval_time', 'number_of_sessions'])\n",
        "df['interval_time'] = df['interval_time'].apply(lambda x: str(x).split()[-1])\n",
        "fig = go.Figure()\n",
        "\n",
        "# строим график\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df['interval_time'],\n",
        "    y=df['number_of_sessions'],\n",
        "    mode='lines+markers',\n",
        "    name='количество сессий'\n",
        "))\n",
        "\n",
        "# Update the layout\n",
        "fig.update_layout(\n",
        "    title='Зависимость количества сессий от выбранного значения таймаута',\n",
        "    xaxis_title='значение таймаута',\n",
        "    yaxis_title='количество сессий'\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n",
        "Таким образом, у нас получилось 10633 сессии.\n",
        "\n",
        "Интервал в 25 минут был выбран на основании того, что с интервалом в 15 минут у нас было бы на 1000 сессий больше, а вот далее, после 25 минутного интервала, количество сессий уменьшалось более медленными темпами - по 500 и менее, а значит наиболее оптимальное время сессии - 25 минут.\n",
        "\n",
        "# возьмем интервал в 25 минут\n",
        "g = (mob_df.groupby('user_id')['event_time'].diff() > pd.Timedelta('25Min')).cumsum()\n",
        "# добавляем столбец с номером сессии\n",
        "mob_df['session_id'] = mob_df.groupby(['user_id', g], sort=False).ngroup() + 1\n",
        "# проверяем получившийся результат\n",
        "mob_df\n",
        "event_time\tevent_name\tuser_id\tsource\tsession_id\n",
        "805\t2019-10-07 13:39:46\ttips_show\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t1\n",
        "806\t2019-10-07 13:40:31\ttips_show\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t1\n",
        "809\t2019-10-07 13:41:06\ttips_show\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t1\n",
        "820\t2019-10-07 13:43:21\ttips_show\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t1\n",
        "830\t2019-10-07 13:45:31\ttips_show\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t1\n",
        "...\t...\t...\t...\t...\t...\n",
        "72584\t2019-11-03 15:51:24\ttips_show\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10633\n",
        "72589\t2019-11-03 15:51:58\tcontacts_show\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10633\n",
        "72684\t2019-11-03 16:07:41\ttips_show\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10633\n",
        "72688\t2019-11-03 16:08:18\ttips_show\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10633\n",
        "72689\t2019-11-03 16:08:25\ttips_show\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10633\n",
        "74197 rows × 5 columns\n",
        "\n",
        "Удалить комменты ревьювера ниже этого поинта\n",
        "Проверка гипотезы: конверсия по сессиям в просмотры контактов различается у пользователей, просматривающих и игнорирующих рекомендации\n",
        "Сначала мы проведем анализ по сессиям, а далее перейдем к сравнению конверсии по двум сегментам - пользователей, просматривающих и игнорирующих рекомендации - это позволит нам выявить, разницу между конверсией по сессиям и по юзерам, что даст нам более широкое представление о поведении юзеров в нашем приложении\n",
        "\n",
        "Расчет конверсии по сессимя\n",
        "\n",
        "# выделяем сессии юзеров, использующих реки\n",
        "tips_session = mob_df.query('event_name == \"tips_click\"')['session_id'].unique()\n",
        "\n",
        "# выделяем сессии юзеров, не использующих реки\n",
        "no_tips_session = mob_df.query('session_id not in @tips_session')['session_id'].unique()\n",
        "\n",
        "# выделяем сессии, в которых юзеры посмотрели контакты\n",
        "contacts_session = mob_df.query('event_name == \"contacts_show\"')['session_id'].unique()\n",
        "\n",
        "# выделяем сессии, в которых юзеры кликнули по рекам и посмотрели контакты\n",
        "tips_contacts_session = set(tips_session) & set(contacts_session)\n",
        "\n",
        "# выделяем сессии, в которых юзеры не кликали по рекам, но посмотрели контакты\n",
        "no_tips_contacts_session = set(no_tips_session) & set(contacts_session)\n",
        "tst = mob_df.copy()\n",
        "tst.sort_values(['session_id', 'event_time'], inplace=True)\n",
        "\n",
        "# удалим повторы в сессиях\n",
        "tst['session_id'] = tst['session_id'].drop_duplicates()\n",
        "tst = tst.dropna()\n",
        "\n",
        "# вернем столбец 'session_id' к типу инт\n",
        "tst['session_id'] = tst['session_id'].astype('int')\n",
        "\n",
        "# добавим столбец 'is_tips' для выделения юзеров, использующих реки\n",
        "tst['is_tips'] =  tst['session_id'].apply(lambda x: True if x in tips_session else False)\n",
        "\n",
        "# добавим столбец 'is_success' для выделения сессий, в которых юзеры просмотрели контакты\n",
        "tst['is_success'] = tst.apply(\n",
        "    lambda row: True if (row['session_id'] in tips_contacts_session)\n",
        "    or (row['session_id'] in no_tips_contacts_session)\n",
        "    else False,\n",
        "    axis=1\n",
        ")\n",
        "# считаем конверсию по сессиям в целевое действие у юзеров, использующих реки\n",
        "tst[tst['is_tips'] == True]['is_success'].mean()\n",
        "0.21770334928229665\n",
        "# считаем конверсию по сессиям в целевое действие у юзеров, не использующих реки\n",
        "tst[tst['is_tips'] == False]['is_success'].mean()\n",
        "0.16250611845325502\n",
        "Как можно заметить, конверсия в просмотр контактов разная, однако без проведения стат. теста мы не можем с уверенностью утверждать, что мы имеем стат. значимое различие, так что переходим к проведению стат. теста\n",
        "\n",
        "H0: конверсия в просмотр контактов для двух сегментов одинаковая\n",
        "\n",
        "H1: конверсия значение просмотр контактов для двух сегментов разная\n",
        "\n",
        "# задаем уровень стат. значимости\n",
        "alpha = .05\n",
        "\n",
        "# проводим стат. тест о равенстве средних\n",
        "results = st.ttest_ind(tst[tst['is_tips'] == True]['is_success'],\n",
        "                       tst[tst['is_tips'] == False]['is_success'], equal_var=False)\n",
        "\n",
        "# выводим получившееся значение p-value и результаты стат. теста\n",
        "print('p-значение:', results.pvalue)\n",
        "\n",
        "if results.pvalue < alpha:\n",
        "    print('Отвергаем нулевую гипотезу')\n",
        "else:\n",
        "    print('Не получилось отвергнуть нулевую гипотезу')\n",
        "p-значение: 0.007462528572232172\n",
        "Отвергаем нулевую гипотезу\n",
        "P-value < 0.05, а значит мы отвергаем нулевую гипотезу и приримаем альтернативную (H1) - \"конверсия в просмотр контактов по сессиям для двух сегментов разная\"\n",
        "\n",
        "Так оно и есть - юзеры, просматривающие наши рекомендации, в среднем активнее более чем в 2 раза - 39 действий против 16.\n",
        "\n",
        "С точки зрения нашей платформы этот инсайт можно трактовать по-разному - с одной стороны, юзеры определенно точно проводят больше времени в приложении, с другой - конверсия в просмотр объявления ниже, а значит наши рекомендации не совсем точны. Или же дело в паттернах поведения?\n",
        "\n",
        "Возможно, юзеры, которые просмтривают наши рекомендации, имеют больше свободного времени, поэтому для них наши рекомендации, являются генератором контента для просмотра.\n",
        "\n",
        "Конверсию в целевое действие - просмотр контактов - по юзерам мы рассмотрим в блоке \"Проработки основных вопросов исследования\"\n",
        "\n",
        "# создадим дф с количеством действий каждого типа\n",
        "# а также посчитаем долю в общем количестве логов для юзеров, использующих рекомендации\n",
        "lovers_evnt = tips_lovers['event_name'].value_counts() \\\n",
        "                         .to_frame() \\\n",
        "                         .reset_index() \\\n",
        "                         .rename(columns={'index': 'event_name', 'event_name': 'am_actions'})\n",
        "lovers_evnt['share'] = lovers_evnt['am_actions'] / lovers_evnt['am_actions'].sum()\n",
        "lovers_evnt = lovers_evnt.sort_values(by='share').reset_index()\n",
        "# создадим дф с количеством действий каждого типа\n",
        "# а также посчитаем долю в общем количестве логов для юзеров, игнорирующих рекомендации\n",
        "haters_evnt = tips_haters['event_name'].value_counts() \\\n",
        "                         .to_frame() \\\n",
        "                         .reset_index() \\\n",
        "                         .rename(columns={'index': 'event_name', 'event_name': 'am_actions'})\n",
        "haters_evnt['share'] = haters_evnt['am_actions'] / haters_evnt['am_actions'].sum()\n",
        "haters_evnt = haters_evnt.sort_values(by='share').reset_index()\n",
        "def actions_share_plot(df, mode):\n",
        "    # построим комбинированный график - столбчатые диаграмммы - количество действий действий каждого типа\n",
        "    # а линия - доля в общем количество действий\n",
        "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "    for i, action in enumerate(df['event_name']):\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=[action],\n",
        "            y=[df.loc[i, 'am_actions']],\n",
        "            name=action,\n",
        "            marker=dict(color=px.colors.qualitative.Pastel2[i % len(px.colors.qualitative.Pastel2)]),\n",
        "            text=[df.loc[i, 'am_actions']],\n",
        "            textposition='inside'\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f'Распределение активности юзеров, {mode} рекомендации',\n",
        "        xaxis=dict(title='название действия в логах'),\n",
        "        legend_title='название действия',\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    fig.update_traces(marker_line_color='black', marker_line_width=1)\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=df['event_name'],\n",
        "        y=df['share'],\n",
        "        mode='lines',\n",
        "        name='доля события',\n",
        "        line=dict(color='red')\n",
        "    ), secondary_y=True)\n",
        "\n",
        "    fig.update_yaxes(title_text='кол-во логов', secondary_y=False)\n",
        "    fig.update_yaxes(title_text='доля в общем кол-ве логов', secondary_y=True)\n",
        "\n",
        "    fig.show()\n",
        "# построим комбинированный график для юзеров, использующих рекомендации\n",
        "actions_share_plot(lovers_evnt, 'использующих')\n",
        "# построим комбинированный график для юзеров, игнорирующих рекомендации\n",
        "actions_share_plot(haters_evnt, 'игнорирующих')\n",
        "Основные различия между пользователями, использующими и игнорирующими рекомендации:\n",
        "\n",
        "для юзеров, использующих рекомендации, доля \"tips_show\" в общем кол-ве событий больше на 20%, 71,4% против 50,4%;\n",
        "для юзеров, игнорирующих рекомендации, вторым наиболее популярным событием в логах, после показа рекомендаций, является просмотр фото с долей 16,2%;\n",
        "стоит заметить, что топ 5 популярных событий в логах для данных двух сегментов различается.\n",
        "Для юзеров, использующих рекомендации, он выглядит следующим образом (название события - доля в общем кол-ве логов):\n",
        "\n",
        "показ рекомендаций - 71,4%;\n",
        "просмотр контактов - 6,9%;\n",
        "клик на рекомендацию - 6,5%;\n",
        "просмотр карты объявлений - 5,3%;\n",
        "открытие карточки объявления - 5,28%.\n",
        "А для юзеров, игнорируюищих рекомендации, он выглядит следующим образом:\n",
        "\n",
        "показ рекомендаций - 50,4%;\n",
        "просмотр фото - 16,2%;\n",
        "поисковая активность - 10,4%;\n",
        "открытие карточки объявления - 8,9%;\n",
        "просмотр контактов - 5,9%.\n",
        "Таким образом, можно сказать, что пользователи, использующие рекомендации, быстрее переходят к просмотру контактов - даже без просмотра фото. Возможными причинами таких паттернов в поведении юзеров может быть:\n",
        "\n",
        "юзеры, использующие реки, проще идут на контакт с продавцами, потому что у них больше свободного времени -> значит больше сделок на таких площадках (по типу Авито) -> проще относятся к коммуникации с продавцами;\n",
        "наши алгоритмы рекомендательной системы настолько хороши, что юзеры могут даже не смотреть фото в объялении и сразу переходя к просмотру контактов;\n",
        "Подытоживая данное сравнение, стоит отметить, что данные показывают, что различия в поведении 2-ух сегментов юзеров - есть, но что более важно, для нас, как для сервиса, юзеры, использующие рекомендации, более привлекательны, поскольку конверсия в просмотр контатков выше -> значит при прочих равных условиях вероятность того, что человек приобретет вещь, выше -> следовательно, можно ожидать приток селлеров, а после него и приток новой части покупателей, ведь на площадке стало больше разных категорий товаров.\n",
        "\n",
        "Анализ источников трафика\n",
        "# посчитаем среднее количество действий на пользователя по каждому источнику привлечения\n",
        "avg_actions = mob_df.groupby(['source', 'user_id'], as_index=False) \\\n",
        "                    ['event_time'].count() \\\n",
        "                    .rename(columns={'event_time': 'am_actions'})\n",
        "for source in avg_actions['source'].unique():\n",
        "  df_t = avg_actions[avg_actions['source'] == source]['am_actions'].mean()\n",
        "  print(f'Среднее кол-во действий пользователей из источника {source} = {round(df_t, 2)}')\n",
        "  print()\n",
        "Среднее кол-во действий пользователей из источника google = 18.11\n",
        "\n",
        "Среднее кол-во действий пользователей из источника other = 15.83\n",
        "\n",
        "Среднее кол-во действий пользователей из источника yandex = 17.73\n",
        "\n",
        "Поскольку среднее значение в нашем случае не столь репрезентативно, то дале мы рассмотрим количестве действий для 25%, 15%, 10% и 5% самых активных пользователя по каждому источнику привлечения и подведем итоги по каждому каналу привлечения\n",
        "\n",
        "# посчитаем количество действий для 25% самых активных пользователя по каждому источнику привлечения\n",
        "for source in avg_actions['source'].unique():\n",
        "  df_t = avg_actions[avg_actions['source'] == source]['am_actions'].quantile(0.75)\n",
        "  print(f'25% самых активных пользователей из источника {source} совершают по {round(df_t, 2)} действий')\n",
        "  print()\n",
        "25% самых активных пользователей из источника google совершают по 18.0 действий\n",
        "\n",
        "25% самых активных пользователей из источника other совершают по 16.0 действий\n",
        "\n",
        "25% самых активных пользователей из источника yandex совершают по 18.0 действий\n",
        "\n",
        "# посчитаем количество действий для 15% самых активных пользователя по каждому источнику привлечения\n",
        "for source in avg_actions['source'].unique():\n",
        "  df_t = avg_actions[avg_actions['source'] == source]['am_actions'].quantile(0.85)\n",
        "  print(f'15% самых активных пользователей из источника {source} совершают по {round(df_t, 2)} действий')\n",
        "  print()\n",
        "15% самых активных пользователей из источника google совершают по 27.8 действий\n",
        "\n",
        "15% самых активных пользователей из источника other совершают по 26.0 действий\n",
        "\n",
        "15% самых активных пользователей из источника yandex совершают по 26.0 действий\n",
        "\n",
        "# посчитаем количество действий для 10% самых активных пользователя по каждому источнику привлечения\n",
        "for source in avg_actions['source'].unique():\n",
        "  df_t = avg_actions[avg_actions['source'] == source]['am_actions'].quantile(0.9)\n",
        "  print(f'10% самых активных пользователей из источника {source} совершают по {round(df_t, 2)} действий')\n",
        "  print()\n",
        "10% самых активных пользователей из источника google совершают по 38.0 действий\n",
        "\n",
        "10% самых активных пользователей из источника other совершают по 34.0 действий\n",
        "\n",
        "10% самых активных пользователей из источника yandex совершают по 36.0 действий\n",
        "\n",
        "# посчитаем количество действий для 5% самых активных пользователя по каждому источнику привлечения\n",
        "for source in avg_actions['source'].unique():\n",
        "  df_t = avg_actions[avg_actions['source'] == source]['am_actions'].quantile(0.95)\n",
        "  print(f'5% самых активных пользователей из источника {source} совершают по {round(df_t, 2)} действий')\n",
        "  print()\n",
        "5% самых активных пользователей из источника google совершают по 58.6 действий\n",
        "\n",
        "5% самых активных пользователей из источника other совершают по 52.55 действий\n",
        "\n",
        "5% самых активных пользователей из источника yandex совершают по 61.35 действий\n",
        "\n",
        "# строим бокс плот по каждому каналу привлечения\n",
        "fig = px.box(avg_actions, x='source', y='am_actions', points='outliers', color='source',\n",
        "             color_discrete_sequence=px.colors.qualitative.Antique)\n",
        "fig.update_layout(\n",
        "    title='Распределение количества действий юзеров по каналам привлечения',\n",
        "    xaxis_title='Источник',\n",
        "    yaxis_title='Количество действий',\n",
        "    legend_title='Канал привлечения',\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "# строим бокс плот по каждому каналу привлечения\n",
        "fig = px.box(avg_actions, x='source', y='am_actions', points='outliers', color='source',\n",
        "             color_discrete_sequence=px.colors.qualitative.Antique)\n",
        "fig.update_layout(\n",
        "    title='Распределение количества действий юзеров по каналам привлечения',\n",
        "    xaxis_title='Источник',\n",
        "    yaxis_title='Количество действий',\n",
        "    legend_title='Канал привлечения',\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.update_yaxes(range=[0, 50])\n",
        "\n",
        "fig.show()\n",
        "Таким образом, можно сказать, что самые активные юзеры приходят либо из \"google\", либо из \"yandex\", однако в среднем метрики по 25, 15, 10 и 5 % самых активных юзеров для всех каналов привлечения +/- одинаковые, поэтому нельзя выделить какой-то один канал, из которого приходят юзеры, перекащивающие распределение количества действий.\n",
        "\n",
        "Далее мы перейдем к визуализации доли целевого трафика по каждому каналу\n",
        "\n",
        "# посчитаем общее кол-во юзеров по каналам\n",
        "total = tst.groupby('source', as_index=False)['user_id'].nunique().rename(columns={'user_id': 'total_am'})\n",
        "\n",
        "# посчитаем кол-во целевых юзеров - просмотревших контакты\n",
        "# по каждому источнику привлечения\n",
        "target = tst[tst['is_success'] == True].groupby('source', as_index=False)['user_id'].nunique().rename(columns={'user_id': 'am_target'})\n",
        "\n",
        "# смержим два дф\n",
        "merged_df = pd.merge(total, target, on='source')\n",
        "\n",
        "# посчитаем долю целевого трафика по каждому каналу\n",
        "merged_df['share_am_target'] = merged_df['am_target'] / merged_df['total_am']\n",
        "merged_df['share_non_target'] = 1 - merged_df['share_am_target']\n",
        "\n",
        "\n",
        "#\n",
        "pie = merged_df.drop(columns=['total_am', 'am_target'])\n",
        "Мы получили таблицу, в которой по каждому источнику есть кол-во и доля юезров, просмотревших контакты, и тех, кто ни разу не посмотрел контакты продавцов.\n",
        "\n",
        "Далее мы построим круговую диаграмму для каждого канала привлечения, на которой отобразим долю целевого трафика - людей, просмотревших контакты - и нецелевого.\n",
        "\n",
        "Доля целевого трафика по каждому каналу\n",
        "# создаем список с названиями источников для итерирования по нему в цикле\n",
        "sources = pie['source'].unique()\n",
        "\n",
        "# задаем размеры сетки с графиками, а также названия графиокв\n",
        "fig = sp.make_subplots(rows=1, cols=3, subplot_titles=sources, specs=[[{'type': 'pie'}]*3])\n",
        "\n",
        "# строим графики\n",
        "for i, source in enumerate(sources):\n",
        "    df_t = (pie[pie['source'] == source]\n",
        "            .set_index('source')\n",
        "            .rename(columns={'share_am_target': 'целевой трафик', 'share_non_target': 'нецелевой трафик'})\n",
        "            .T)\n",
        "    labels = df_t.index.values\n",
        "    values = df_t[source]\n",
        "\n",
        "    pie_trace = go.Pie(labels=labels, values=values, textinfo='percent',\n",
        "                       marker=dict(line=dict(color='black', width=1)),\n",
        "                       pull=[0, 0.3],\n",
        "                       marker_colors=px.colors.qualitative.Pastel2)\n",
        "\n",
        "    fig.add_trace(pie_trace, row=1, col=i+1)\n",
        "    fig.update_layout(title_text='Доли целевого и нецелевого трафика внутри канала привлечения')\n",
        "\n",
        "fig.update_layout(showlegend=True, legend=dict(orientation='h'))\n",
        "\n",
        "fig.show()\n",
        "По графику выше видно, что доля пользователей, просмотревших контакты, больше всего в источнике – \"yandex\" - 24,7%, чуть меньше в \"google\" c 24,4%, а наименьшая доля в \"other\" - лишь 18,5%.\n",
        "\n",
        "Таким образом, если мы хотим растить долю пользователей, запрашивающих контакты, то в первую очередь стоит смотреть на масштабирование каналов \"yandex\" или \"google\", при условии, что стоимость привлечения одинаковая для всех каналов.\n",
        "\n",
        "Итоги анализа каналов привлечения\n",
        "\n",
        "Можно сказать, что в целом, все юзеров со всех каналов +/- одинаковые, однако с учетом того, что самые активные юзеры приходят не с источника \"other\", а с \"google\" или с \"yandex\", тем более, в этих 2-ух каналах доля целевого трафика - юзеров, просмотривающих контакты - почти одинаковая;\n",
        "\n",
        "если выбирать канал для масштабирования, то это опредленно должен быть либо \"yandex\", либо \"google\", при условии, что рекламный рынок во всех 3 каналах +/- одинаковый.\n",
        "\n",
        "Шаг 4. Проработка основных вопросов исследования\n",
        "Выделение наиболее популярных сценариев использования приложения\n",
        "# переименуем столбец, что сохранить информацию об источнике трафика\n",
        "mob_df = mob_df.rename(columns={'source': 'traff_source'})\n",
        "Начнем с определения наиболее популярных сценариев. Для решения данной задачи была прочитана статья на хабре - https://habr.com/en/articles/566568/ - и код из статьи использовался в качестве шаблона, однако на этапе применения функции \"add_features\" добавляем условие на удаление строк, в которых совпадает \"source\" и \"target\", а также немного оптимизируем процесс удаления строк, в которых юзеры совершили более 4 действий за сессию. Раньше мы просто удаляли записи со значением в столбце \"step\" больше 4, а теперь мы будем удалять все записи, относящиеся к таким сессиям - для этого напишем функцию drop_sessions\n",
        "\n",
        "def add_features(df):\n",
        "    sorted_df = df.sort_values(by=['session_id', 'event_time'])\n",
        "    sorted_df['step'] = sorted_df.groupby('session_id').cumcount() + 1\n",
        "    sorted_df['source'] = sorted_df['event_name']\n",
        "    sorted_df['target'] = sorted_df.groupby('session_id')['source'].shift(-1)\n",
        "\n",
        "    # пропишем условие на сохранение строки с уникальными значениями в столбце 'target' для каждой сессии\n",
        "    unique_steps_df = sorted_df.drop_duplicates(subset=['session_id','target'], keep='first')\n",
        "\n",
        "    return unique_steps_df.drop(['event_name'], axis=1)\n",
        "# применим вышеописанную функцию к нашему дф\n",
        "table = add_features(mob_df)\n",
        "# применим условие к полученной таблице\n",
        "new_df = table[table['source'] != table['target']]\n",
        "new_df\n",
        "event_time\tuser_id\ttraff_source\tsession_id\tstep\tsource\ttarget\n",
        "839\t2019-10-07 13:49:42\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t1\t9\ttips_show\tNaN\n",
        "6546\t2019-10-09 18:35:28\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t2\t2\tmap\ttips_show\n",
        "6566\t2019-10-09 18:42:23\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t2\t4\ttips_show\tNaN\n",
        "36416\t2019-10-21 19:53:17\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t3\t2\ttips_show\tmap\n",
        "36486\t2019-10-21 20:07:30\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t3\t14\ttips_show\tNaN\n",
        "...\t...\t...\t...\t...\t...\t...\t...\n",
        "72321\t2019-11-03 14:32:56\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10632\t1\ttips_show\tcontacts_show\n",
        "72325\t2019-11-03 14:33:48\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10632\t2\tcontacts_show\ttips_show\n",
        "72364\t2019-11-03 14:48:44\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10632\t15\ttips_show\tNaN\n",
        "72547\t2019-11-03 15:47:44\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10633\t5\ttips_show\tcontacts_show\n",
        "72689\t2019-11-03 16:08:25\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10633\t14\ttips_show\tNaN\n",
        "19291 rows × 7 columns\n",
        "\n",
        "Что получили в итоговой таблице:\n",
        "\n",
        "события каждого id отсортированы по времени;\n",
        "созданы пары событий source - target;\n",
        "добавлен шаг между этими событиями для построения диаграммы;\n",
        "удален столбец event_name, так как в дальнейших преобразованиях он использоваться не будет.\n",
        "Далее мы определим, сколько шагов (этапов) лучше всего выбрать для нашего CJM - то есть мы посмотрим на основную стат. информацию по столбцу с кол-во шагов в сессии\n",
        "\n",
        "# выводим основную стат. информацию по столбцу с кол-во шагов в сессии\n",
        "new_df['step'].describe()\n",
        "count    19291.000000\n",
        "mean         5.405681\n",
        "std          7.813883\n",
        "min          1.000000\n",
        "25%          1.000000\n",
        "50%          3.000000\n",
        "75%          6.000000\n",
        "max        139.000000\n",
        "Name: step, dtype: float64\n",
        "Таким образом, оптимальнее всего будет выбрать 4 шагов, поскольку свыше этого значения, лежат сессии меньшей части пользователей (менее 50%)\n",
        "\n",
        "def drop_sessions(df):\n",
        "    session_max_step = df.groupby('session_id')['step'].max()\n",
        "    sessions_to_drop = session_max_step[session_max_step > 4].index\n",
        "    return df[~df['session_id'].isin(sessions_to_drop)].reset_index(drop=True)\n",
        "\n",
        "# применим фнукцию, которая исключит сессии, в которых больше 4 шагов - сессии меньшей части юзеров\n",
        "df_comp = drop_sessions(new_df)\n",
        "df_comp\n",
        "event_time\tuser_id\ttraff_source\tsession_id\tstep\tsource\ttarget\n",
        "0\t2019-10-09 18:35:28\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t2\t2\tmap\ttips_show\n",
        "1\t2019-10-09 18:42:23\t0001b1d5-b74a-4cbf-aeb0-7df5947bf349\tother\t2\t4\ttips_show\tNaN\n",
        "2\t2019-10-20 20:04:17\t00157779-810c-4498-9e05-a1e9e3cedf93\tyandex\t7\t2\tphotos_show\tcontacts_show\n",
        "3\t2019-10-20 20:05:36\t00157779-810c-4498-9e05-a1e9e3cedf93\tyandex\t7\t4\tphotos_show\tNaN\n",
        "4\t2019-11-03 17:12:10\t00157779-810c-4498-9e05-a1e9e3cedf93\tyandex\t11\t1\tcontacts_show\tNaN\n",
        "...\t...\t...\t...\t...\t...\t...\t...\n",
        "7775\t2019-11-02 18:01:27\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10630\t1\ttips_show\tcontacts_show\n",
        "7776\t2019-11-02 18:17:41\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10630\t2\tcontacts_show\tNaN\n",
        "7777\t2019-11-02 19:25:54\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10631\t1\ttips_show\tcontacts_show\n",
        "7778\t2019-11-02 19:26:08\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10631\t2\tcontacts_show\ttips_show\n",
        "7779\t2019-11-02 19:30:50\tfffb9e79-b927-4dbb-9b48-7fd09b23a62b\tgoogle\t10631\t4\ttips_show\tNaN\n",
        "7780 rows × 7 columns\n",
        "\n",
        "def get_source_index(df):\n",
        "\n",
        "    \"\"\"Функция генерации индексов source\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): исходная таблица с признаками step, source, target.\n",
        "    Returns:\n",
        "        dict: словарь с индексами, именами и соответсвиями индексов именам source.\n",
        "    \"\"\"\n",
        "\n",
        "    res_dict = {}\n",
        "\n",
        "    count = 0\n",
        "    # получаем индексы источников\n",
        "    for no, step in enumerate(df['step'].unique().tolist()):\n",
        "        # получаем уникальные наименования для шага\n",
        "        res_dict[no+1] = {}\n",
        "        res_dict[no+1]['sources'] = df[df['step'] == step]['source'].unique().tolist()\n",
        "        res_dict[no+1]['sources_index'] = []\n",
        "        for i in range(len(res_dict[no+1]['sources'])):\n",
        "            res_dict[no+1]['sources_index'].append(count)\n",
        "            count += 1\n",
        "\n",
        "    # соединим списки\n",
        "    for key in res_dict:\n",
        "        res_dict[key]['sources_dict'] = {}\n",
        "        for name, no in zip(res_dict[key]['sources'], res_dict[key]['sources_index']):\n",
        "            res_dict[key]['sources_dict'][name] = no\n",
        "    return res_dict\n",
        "# создаем словарь\n",
        "source_indexes = get_source_index(df_comp)\n",
        "def generate_random_color():\n",
        "\n",
        "    \"\"\"Случайная генерация цветов rgba\n",
        "\n",
        "    Args:\n",
        "\n",
        "    Returns:\n",
        "        str: Строка со сгенерированными параметрами цвета\n",
        "    \"\"\"\n",
        "\n",
        "    # сгенерим значение для каждого канала\n",
        "    r, g, b = np.random.randint(255, size=3)\n",
        "    return f'rgba({r}, {g}, {b}, 1)'\n",
        "def colors_for_sources(mode):\n",
        "\n",
        "    \"\"\"Генерация цветов rgba\n",
        "\n",
        "    Args:\n",
        "        mode (str): сгенерировать случайные цвета, если 'random', а если 'custom' -\n",
        "                    использовать заранее подготовленные\n",
        "    Returns:\n",
        "        dict: словарь с цветами, соответствующими каждому индексу\n",
        "    \"\"\"\n",
        "    # словарь, в который сложим цвета в соответствии с индексом\n",
        "    colors_dict = {}\n",
        "\n",
        "    if mode == 'random':\n",
        "        # генерим случайные цвета\n",
        "        for label in df_comp['source'].unique():\n",
        "            r, g, b = np.random.randint(255, size=3)\n",
        "            colors_dict[label] = f'rgba({r}, {g}, {b}, 1)'\n",
        "\n",
        "    elif mode == 'custom':\n",
        "        # присваиваем ранее подготовленные цвета\n",
        "        colors = requests.get('https://raw.githubusercontent.com/rusantsovsv/senkey_tutorial/main/json/colors_senkey.json').json()\n",
        "        for no, label in enumerate(df_comp['source'].unique()):\n",
        "            colors_dict[label] = colors['custom_colors'][no]\n",
        "\n",
        "    return colors_dict\n",
        "# генерим цвета из своего списка\n",
        "colors_dict = colors_for_sources(mode='random')\n",
        "def percent_users(sources, targets, values):\n",
        "\n",
        "    \"\"\"\n",
        "    Расчет уникальных id в процентах (для вывода в hover text каждого узла)\n",
        "\n",
        "    Args:\n",
        "        sources (list): список с индексами source.\n",
        "        targets (list): список с индексами target.\n",
        "        values (list): список с \"объемами\" потоков.\n",
        "\n",
        "    Returns:\n",
        "        list: список с \"объемами\" потоков в процентах\n",
        "    \"\"\"\n",
        "\n",
        "    # объединим источники и метки и найдем пары\n",
        "    zip_lists = list(zip(sources, targets, values))\n",
        "\n",
        "    new_list = []\n",
        "\n",
        "    # подготовим список словарь с общим объемом трафика в узлах\n",
        "    unique_dict = {}\n",
        "\n",
        "    # проходим по каждому узлу\n",
        "    for source, target, value in zip_lists:\n",
        "        if source not in unique_dict:\n",
        "            # находим все источники и считаем общий трафик\n",
        "            unique_dict[source] = 0\n",
        "            for sr, tg, vl in zip_lists:\n",
        "                if sr == source:\n",
        "                    unique_dict[source] += vl\n",
        "\n",
        "    # считаем проценты\n",
        "    for source, target, value in zip_lists:\n",
        "        new_list.append(round(100 * value / unique_dict[source], 1))\n",
        "\n",
        "    return new_list\n",
        "def lists_for_plot(source_indexes=source_indexes, colors=colors_dict, frac=10):\n",
        "\n",
        "    \"\"\"\n",
        "    Создаем необходимые для отрисовки диаграммы переменные списков и возвращаем\n",
        "    их в виде словаря\n",
        "\n",
        "    Args:\n",
        "        source_indexes (dict): словарь с именами и индексами source.\n",
        "        colors (dict): словарь с цветами source.\n",
        "        frac (int): ограничение на минимальный \"объем\" между узлами.\n",
        "\n",
        "    Returns:\n",
        "        dict: словарь со списками, необходимыми для диаграммы.\n",
        "    \"\"\"\n",
        "\n",
        "    sources = []\n",
        "    targets = []\n",
        "    values = []\n",
        "    labels = []\n",
        "    link_color = []\n",
        "    link_text = []\n",
        "\n",
        "    # проходим по каждому шагу\n",
        "    for step in tqdm(sorted(df_comp['step'].unique()), desc='Шаг'):\n",
        "        if step + 1 not in source_indexes:\n",
        "            continue\n",
        "\n",
        "        # получаем индекс источника\n",
        "        temp_dict_source = source_indexes[step]['sources_dict']\n",
        "\n",
        "        # получаем индексы цели\n",
        "        temp_dict_target = source_indexes[step+1]['sources_dict']\n",
        "\n",
        "        # проходим по каждой возможной паре, считаем количество таких пар\n",
        "        for source, index_source in tqdm(temp_dict_source.items()):\n",
        "            for target, index_target in temp_dict_target.items():\n",
        "                # делаем срез данных и считаем количество id\n",
        "                temp_df = df_comp[(df_comp['step'] == step)&(df_comp['source'] == source)&(df_comp['target'] == target)]\n",
        "                value = len(temp_df)\n",
        "                # проверяем минимальный объем потока и добавляем нужные данные\n",
        "                if value > frac:\n",
        "                    sources.append(index_source)\n",
        "                    targets.append(index_target)\n",
        "                    values.append(value)\n",
        "                    # делаем поток прозрачным для лучшего отображения\n",
        "                    link_color.append(colors[source].replace(', 1)', ', 0.2)'))\n",
        "\n",
        "    labels = []\n",
        "    colors_labels = []\n",
        "    for key in source_indexes:\n",
        "        for name in source_indexes[key]['sources']:\n",
        "            labels.append(name)\n",
        "            colors_labels.append(colors[name])\n",
        "\n",
        "    # посчитаем проценты всех потоков\n",
        "    perc_values = percent_users(sources, targets, values)\n",
        "\n",
        "    # добавим значения процентов для howertext\n",
        "    link_text = []\n",
        "    for perc in perc_values:\n",
        "        link_text.append(f\"{perc}%\")\n",
        "\n",
        "    # возвратим словарь с вложенными списками\n",
        "    return {'sources': sources,\n",
        "            'targets': targets,\n",
        "            'values': values,\n",
        "            'labels': labels,\n",
        "            'colors_labels': colors_labels,\n",
        "            'link_color': link_color,\n",
        "            'link_text': link_text}\n",
        "# создаем словарь\n",
        "data_for_plot = lists_for_plot()\n",
        "Шаг:   0%|          | 0/4 [00:00<?, ?it/s]\n",
        "  0%|          | 0/9 [00:00<?, ?it/s]\n",
        "  0%|          | 0/9 [00:00<?, ?it/s]\n",
        "  0%|          | 0/8 [00:00<?, ?it/s]\n",
        "def plot_senkey_diagram(data_dict=data_for_plot):\n",
        "\n",
        "    \"\"\"\n",
        "    Функция для генерации объекта диаграммы Сенкей\n",
        "\n",
        "    Args:\n",
        "        data_dict (dict): словарь со списками данных для построения.\n",
        "\n",
        "    Returns:\n",
        "        plotly.graph_objs._figure.Figure: объект изображения.\n",
        "    \"\"\"\n",
        "\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        domain = dict(\n",
        "          x =  [0,1],\n",
        "          y =  [0,1]\n",
        "        ),\n",
        "        orientation = \"h\",\n",
        "        valueformat = \".0f\",\n",
        "        node = dict(\n",
        "          pad = 50,\n",
        "          thickness = 15,\n",
        "          line = dict(color = \"black\", width = 0.1),\n",
        "          label = data_dict['labels'],\n",
        "          color = data_dict['colors_labels']\n",
        "        ),\n",
        "        link = dict(\n",
        "          source = data_dict['sources'],\n",
        "          target = data_dict['targets'],\n",
        "          value = data_dict['values'],\n",
        "          label = data_dict['link_text'],\n",
        "          color = data_dict['link_color']\n",
        "      ))])\n",
        "    fig.update_layout(title_text=\"Sankey Diagram\", font_size=10, width=1050, height=700)\n",
        "\n",
        "    # возвращаем объект диаграммы\n",
        "    return fig\n",
        "# сохраняем диаграмму в переменную\n",
        "senkey_diagram = plot_senkey_diagram()\n",
        "senkey_diagram.show()\n",
        "# обновим фнукцию, написанную выше, для создания дф\n",
        "# для анализа прошедшего времени между \"tips_click\" и \"contacts_show\"\n",
        "def add_features2(df):\n",
        "    sorted_df = df.sort_values(by=['session_id', 'event_time'])\n",
        "    sorted_df['step'] = sorted_df.groupby('session_id').cumcount() + 1\n",
        "    sorted_df['source'] = sorted_df['event_name']\n",
        "    sorted_df['target'] = sorted_df.groupby('user_id')['source'].shift(-1)\n",
        "\n",
        "    return sorted_df\n",
        "sessions_df = add_features2(mob_df)\n",
        "Переходим к анализу сессий, в которых \"contacts_show\" было совершено вторым действием\n",
        "\n",
        "# напишем функцию для построения воронок для двухэтапных сессий\n",
        "def generate_funnel_plot(actions, sessions_df):\n",
        "    base_event_df = (mob_df[mob_df['event_name'] == actions]\n",
        "                     .groupby('user_id', as_index=False)\n",
        "                     .agg(base_event_time=('event_time', 'min')))\n",
        "\n",
        "    base_event_users = base_event_df['user_id'].nunique()\n",
        "\n",
        "    contacts_user_df = (mob_df[mob_df['event_name'] == 'contacts_show']\n",
        "                        .merge(base_event_df, on='user_id', how='left'))\n",
        "\n",
        "    contacts_user_df = contacts_user_df[contacts_user_df['event_time'] > contacts_user_df['base_event_time']]\n",
        "\n",
        "    target_event_users = contacts_user_df['user_id'].nunique()\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Funnel(\n",
        "        y=[actions, 'contacts_show'],\n",
        "        x=[base_event_users, target_event_users],\n",
        "        textposition=\"inside\",\n",
        "        textinfo=\"value+percent initial+percent previous\",\n",
        "        name=f\"{actions} Funnel\"\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(title=f'Двухэтапная воронка с началом в {actions}', showlegend=False)\n",
        "\n",
        "    fig.update_layout(\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        paper_bgcolor='rgba(0,0,0,0)'\n",
        "    )\n",
        "\n",
        "    fig.update_xaxes(showgrid=False)\n",
        "    fig.update_yaxes(showgrid=False)\n",
        "\n",
        "    fig.show()\n",
        "# построим воронку с началом в \"tips_show\"\n",
        "generate_funnel_plot('tips_show', sessions_df)\n",
        "# построим воронку с началом в \"search\"\n",
        "generate_funnel_plot('search', sessions_df)\n",
        "# построим воронку с началом в \"photos_show\"\n",
        "generate_funnel_plot('photos_show', sessions_df)\n",
        "Таким образом, по воронкам выше можно понять, что среди юзеров, совершивших целевое действие на втором шаге (второй лог внутри сессии) - просмотр контактов, наиболее популярным сценарием использования нашего приложения является переход от показа рекомендаций (\"tips_show\"). У 2801 нашего юзера сессия начиналась с этого события, и лишь 495 далее просмотрели контакты - конверсия в целевое действие составила 17,7% на 2 этапе. Следующим по популярности сцеанрием является переход от поисковой активности (\"search\") к просмотру контактов - из 1666 юзеров, у которых сессии начались с поиска, целевое действие совершили 342 пользователя, то есть конверсия в просмотр контактов на втором шаге в данном сценарии - 20,5%. И третим по популярности сценарием является переход от просмотра фото (\"photos_show\") к запросу контактов. Так, из 1095 юзеров просмотрели контакты 301 пользователь - конверсия целых 27,5%.\n",
        "\n",
        "Подводя итог анализу двухэтапных пользовательских сценариев, стоит отметить следующие моменты:\n",
        "\n",
        "чем более популярен сценрий - тем меньше конверсия в целевое действие (варьируется от 17,7% до 27,5%);\n",
        "связать данную тенденцию можно с тем, что чем более популярен сценарий, тем менее ярко выражен запрос у покупателя на поиск какого-либо товара - это подтверждается и событием, с которого начинаются данные воронки;\n",
        "самый популярный, но с наименьшей конверсией - сценарий, начинающийся с рекомендаций;\n",
        "наименее популярный, но с наибольшей конверсией - переход от просмотра фото к запросу контактов.\n",
        "Таким образом, в этой ситуации можно было сделать ошибочный вывод - если мы сможем сконвертить юзеров в просмотр фото после клика на рекомендацию, то мы сможем увеличить конверсию в просмотр контатков. Но дело в том, что люди, которые смотрят фото, скорее всего, совершают данное действие поскольку у них \"болит\" потребность в данной услуге / товаре, а вот юзеры, кликающие на реки, могут просто сидеть в ленте рекомендаций, даже не имея цели что-то приобрести. Но данные предположения мы сможем проверить лишь с помощью проведения глубинных (пользовательских) интервью.\n",
        "\n",
        "# определим ай ди сессий, в которых кол-во шагов было 1 или 2\n",
        "two_steps_sess = sessions_df[sessions_df['step'] < 3]\n",
        "\n",
        "# определим события, из которых мы считаем конверсию в целевое действие - \"contacts_show\"\n",
        "actions = ['tips_show', 'search', 'photos_show']\n",
        "\n",
        "# пишем цикл с подсчетом общего кол-ва сессий с каждым действием из списка\n",
        "# а также считаем кол-во сессий с целевым действие и конверсию\n",
        "for action in actions:\n",
        "  total_sess = sessions_df[sessions_df['source'] == action]['session_id'].nunique()\n",
        "  target_sess = two_steps_sess[(two_steps_sess['source'] == action)\n",
        "             & (two_steps_sess['target'] == 'contacts_show')]['session_id'].nunique()\n",
        "  cr = round(target_sess / total_sess, 4)\n",
        "  print(f'Общее количество сессий с действием {action} составило {total_sess}')\n",
        "  print(f'Всего {target_sess} сессий с действием {action}, которые закончились целевым действие')\n",
        "  print(f'Конверсия в целевое действие составила {cr*100}%')\n",
        "  print()\n",
        "Общее количество сессий с действием tips_show составило 6189\n",
        "Всего 348 сессий с действием tips_show, которые закончились целевым действие\n",
        "Конверсия в целевое действие составила 5.62%\n",
        "\n",
        "Общее количество сессий с действием search составило 2997\n",
        "Всего 146 сессий с действием search, которые закончились целевым действие\n",
        "Конверсия в целевое действие составила 4.87%\n",
        "\n",
        "Общее количество сессий с действием photos_show составило 2571\n",
        "Всего 159 сессий с действием photos_show, которые закончились целевым действие\n",
        "Конверсия в целевое действие составила 6.18%\n",
        "\n",
        "Переходим к анализу сессий, в которых \"contacts_show\" было совершено третьим действием и на этом мы закончим детальное рассмотрение сессий\n",
        "\n",
        "# создадим функцию для построения воронок для сессий, в которых\n",
        "# целевое действие - \"contacts_show\" - было совершено на 3 шаге\n",
        "def three_steps_funnel_plot(mid_action, sources_mid_act, sessions_df):\n",
        "    # рисуем сетку для будущих графиков\n",
        "    fig = make_subplots(rows=1, cols=len(sources_mid_act), subplot_titles=sources_mid_act, horizontal_spacing=0.2)\n",
        "\n",
        "    # подготавливаем данные для будущих графиков\n",
        "    for i, source_mid_action in enumerate(sources_mid_act, start=1):\n",
        "        total_usr_id = sessions_df[sessions_df['source'] == source_mid_action]['user_id'].unique()\n",
        "        mid_act = sessions_df[(sessions_df['user_id'].isin(total_usr_id)) &\n",
        "                              (sessions_df['source'] == mid_action)]['user_id'].unique()\n",
        "        target_am = sessions_df[(sessions_df['user_id'].isin(mid_act)) &\n",
        "                                (sessions_df['event_name'] == 'contacts_show')]['user_id'].nunique()\n",
        "\n",
        "        # создаем воронку\n",
        "        fig.add_trace(go.Funnel(\n",
        "            y=[source_mid_action, mid_action, 'contacts_show'],\n",
        "            x=[len(total_usr_id), len(mid_act), target_am],\n",
        "            textposition=\"inside\",\n",
        "            textinfo=\"value+percent initial+percent previous\",\n",
        "            name=f\"{source_mid_action} Funnel\"\n",
        "        ), row=1, col=i)\n",
        "\n",
        "        # меняем цвет бэкграунда\n",
        "        fig.update_layout(\n",
        "            plot_bgcolor='rgba(0,0,0,0)',\n",
        "            paper_bgcolor='rgba(0,0,0,0)'\n",
        "        )\n",
        "\n",
        "        # убираем сетку\n",
        "        fig.update_xaxes(showgrid=False)\n",
        "        fig.update_yaxes(showgrid=False)\n",
        "\n",
        "    fig.update_layout(title='Трехэтапные воронки', showlegend=False)\n",
        "\n",
        "    fig.show()\n",
        "# создадим список источников для промежуточного действия \"tips_show\"\n",
        "sources_mid_act = ['map', 'advert_open', 'search']\n",
        "# построим воронку с промежуточным действием \"tips_show\"\n",
        "three_steps_funnel_plot('tips_show', sources_mid_act, sessions_df)\n",
        "# создадим список источников для промежуточного действия \"photos_show\"\n",
        "sources_mid_act = ['advert_open', 'search']\n",
        "# построим воронку с промежуточным действием \"photos_show\"\n",
        "three_steps_funnel_plot('photos_show', sources_mid_act, sessions_df)\n",
        "Переходя к анализу трехэтапных воронок, можно заметить, что конверсия в целевое действие по всем сценариям сильно ниже по сравнению с двуххэтапными сценариями.\n",
        "\n",
        "Так среди юзеров, совершивших целевое действие - просмотр контактов - на третьем шаге (третий лог внутри сессии) после просмотра рекомендаций (второй лог) наиболее популярным сценарием использования нашего приложения является переход от поиска (\"search\"). У 1666 наших юзеров сессия начиналась с этого события, из них 801 просмотрели рекомендации, а до просмотра контактов дошли лищь 139 пользователей. То есть конверсия в целевое действие составила 8,3% на 3 шаге. Следующим по популярности сцеанрием является переход от просмтотра карты (\"map\") к просмотру рекомендаций - из 1456 юзеров, у которых сессии начались с открытия карты, далее перешло к просмотру рекомендаций 1352 пользователя, а из них 275 перешли к просмотру контактов, то есть конверсия в целевое действие на третьем шаге в данном сценарии - 18,9%. И третим по популярности сценарием является переход от просмотра объявлений (\"advert_open\"), далее просмотор рек, а потом уже запрос контактов. Так, из 751 юзеров, открывших объявление, к просмотру рекомендаций перешло 590 человек, из них 88 просмотрели контакты - конверсия 11,7%.\n",
        "\n",
        "Переходим к воронкам, у которых промежуточным действием было просмотр фото, тут мы рассмотрим 2 сценария. Так, в первом случае из 1666 наших юзеров, начавших с поиска, 647 просмотрели фото, а до просмотра контактов дошли 195 пользователей. То есть конверсия в целевое действие составила 11,5% на 3 шаге. Во втором случае из 751 юзера, открывших объявление, к просмотру фото перешло лишь 73 человека, а до просмотра контактов дошли всего лишь 38 юзеров - то есть конверсия данного сценария - 5,3%.\n",
        "\n",
        "Подводя итог анализу трехэтапных пользовательских сценариев, стоит отметить следующие моменты:\n",
        "\n",
        "в данном случае у нас нет тендции к тому, чтобы сценарии с наименьшей базой юезров на первом этапе получали наибольшую конверсию в целевое действие в итоге;\n",
        "наиболее популярным сценарием в данном случае является тот, что начинается с поисковой активности, если смотреть по началу воронки;\n",
        "а вот наибольшей популярностью с точки зрения количества юзеров, совершивших целевое действие, является сценарий, начавшийся с просмотра карты;\n",
        "так, в случае со сценарием \"map\" -> \"photos_show\" -> \"contacts_show\" конверсия в целевое действие составила целых 18,9% - что является наибольшим значением, среди всех сценариев, в которых контакты просмотрели на 3 шаге;\n",
        "наименее популярный сценарий - \"advert_open\" -> \"photos_show\" -> \"contacts_show\" - 38 юзеров совершили целевое действие или же лишь 5,3% от начала воронки.\n",
        "Выводом, обобщающим анализ пользовательских сценариев, может стать факт того, что чем меньше шагов до целевого события, тем выше конверсия в его совершение. Как это можно применить? Как мне кажется, проведение пользовательских интервью для каждого сегмента юзеров - тех, кто обычно приходит в приложение с конкретным запросом (сессии начинаются с \"search\"); тех, кто готов полистать наши рекомендации (сессии начинаются с \"tips_show\" или содержат много данных логов внутри сессии) и для остальных сегментов, рассмотренных выше - что позволит спроектировать сценарии использования с учетом особенностей каждого сегмента, что в конечном итоге может приветси к сокращению средней длины пути до целевого действия.\n",
        "\n",
        "# создадим переменную с сессиями до 3-х шага\n",
        "three_steps_sess = sessions_df[sessions_df['step'] < 4]\n",
        "\n",
        "# напишем функцию, основанную на идеи выше, для расчета конверсии в целевое действие по сессиям\n",
        "# для 3 этапных сессий, в которых переход к просмотру контактов произошел от показа рекомендации\n",
        "def three_steps_cr(mid_action, action):\n",
        "  act_sess = three_steps_sess[(three_steps_sess['step'] == 1) &\n",
        "                              (three_steps_sess['source'] == action)]['session_id'].unique()\n",
        "  act_target_sess = (three_steps_sess[three_steps_sess['session_id'].isin(act_sess)]\n",
        "                                    [((three_steps_sess['step'] == 3) &\n",
        "                                      (three_steps_sess['source'] == mid_action) &\n",
        "                                    (three_steps_sess['target'] == 'contacts_show'))]['session_id'].nunique())\n",
        "  act_cr = round(act_target_sess / len(act_sess), 4)\n",
        "  print(f'Общее количество сессий с действием {action} составило {len(act_sess)}')\n",
        "  print(f'Всего {act_target_sess} сессий с действием {action}, которые закончились целевым действие')\n",
        "  print(f'Конверсия в целевое действие составила {act_cr*100}%')\n",
        "# Считаем конверсию для сессий, начавшихся с \"map\"\n",
        "# и промежуточным действием \"tips_show\"\n",
        "three_steps_cr('tips_show', 'map')\n",
        "Общее количество сессий с действием map составило 1306\n",
        "Всего 20 сессий с действием map, которые закончились целевым действие\n",
        "Конверсия в целевое действие составила 1.53%\n",
        "# Считаем конверсию для сессий, начавшихся с \"advert_open\"\n",
        "# и промежуточным действием \"tips_show\"\n",
        "three_steps_cr('tips_show', 'advert_open')\n",
        "Общее количество сессий с действием advert_open составило 425\n",
        "Всего 2 сессий с действием advert_open, которые закончились целевым действие\n",
        "Конверсия в целевое действие составила 0.47000000000000003%\n",
        "# Считаем конверсию для сессий, начавшихся с \"search\"\n",
        "# и промежуточным действием \"tips_show\"\n",
        "three_steps_cr('tips_show', 'search')\n",
        "Общее количество сессий с действием search составило 2517\n",
        "Всего 9 сессий с действием search, которые закончились целевым действие\n",
        "Конверсия в целевое действие составила 0.36%\n",
        "# Считаем конверсию для сессий, начавшихся с \"advert_open\"\n",
        "# и промежуточным действием \"photos_show\"\n",
        "three_steps_cr('photos_show', 'advert_open')\n",
        "Общее количество сессий с действием advert_open составило 425\n",
        "Всего 1 сессий с действием advert_open, которые закончились целевым действие\n",
        "Конверсия в целевое действие составила 0.24%\n",
        "# Считаем конверсию для сессий, начавшихся с \"search\"\n",
        "# и промежуточным действием \"photos_show\"\n",
        "three_steps_cr('photos_show', 'search')\n",
        "Общее количество сессий с действием search составило 2517\n",
        "Всего 14 сессий с действием search, которые закончились целевым действие\n",
        "Конверсия в целевое действие составила 0.5599999999999999%\n",
        "Насколько хороши наши рекомендации?\n",
        "Проверка гипотезы: \"конверсия в просмотры контактов различается у юзеров, совершающих действия tips_show и tips_click, отличается от — только tips_show\"\n",
        "# выделяем айди юзеров, совершающих \"tips_show\"\n",
        "tips_show_usr = mob_df[mob_df['event_name'] == 'tips_show']['user_id'].unique()\n",
        "\n",
        "# выделяем айди юзеров, совершающих \"tips_show\" и \"tips_click\"\n",
        "tips_usr = (mob_df[(mob_df['event_name'] == 'tips_click') &\n",
        "                   (mob_df['user_id'].isin(tips_show_usr))])\n",
        "\n",
        "# выделяем айди юзеров, которые посмотрели контакты и совершали  \"tips_show\" и \"tips_click\"\n",
        "click_contacts = mob_df[(mob_df['user_id'].isin(tips_usr['user_id'])) &\n",
        "                             (mob_df['event_name'] == 'contacts_show')]\n",
        "\n",
        "# выделяем айди юзеров, совершающих только \"tips_show\"\n",
        "# и исключаем пересечения с теми, кто совершает \"tips_show\" и \"tips_click\"\n",
        "no_tips_usr = (mob_df[(mob_df['event_name'] == 'tips_show') &\n",
        "                      (~mob_df['user_id'].isin(tips_usr['user_id']))])\n",
        "\n",
        "# выделяем среди юзеров, совершающих только \"tips_show\", тех, кто посмотрел контакты\n",
        "no_tips_contacts = mob_df[(mob_df['user_id'].isin(no_tips_usr['user_id'])) &\n",
        "                             (mob_df['event_name'] == 'contacts_show')]\n",
        "# считаем конверсию в целевое действие у юзеров, использующих реки\n",
        "click_contacts['user_id'].nunique() / tips_usr['user_id'].nunique()\n",
        "0.3063973063973064\n",
        "# считаем конверсию в целевое действие у юзеров, не использующих реки\n",
        "no_tips_contacts['user_id'].nunique() / no_tips_usr['user_id'].nunique()\n",
        "0.16972843450479233\n",
        "Как можно заметить, конверсия в просмотр контактов разная, однако без проведения стат. теста мы не можем с уверенностью утверждать, что мы имеем стат. значимое различие, так что переходим к проведению стат. теста\n",
        "\n",
        "# добавим в дф с юзерами, использующими рекомендации,\n",
        "# столбец по которому мы будем проводить стат тест\n",
        "tips_usr['is_success'] = tips_usr['user_id'].isin(click_contacts['user_id'])\n",
        "\n",
        "# добавим в дф с юзерами, не использующими рекомендации,\n",
        "# столбец, по которому мы будем проводить стат тест\n",
        "no_tips_usr['is_success'] = no_tips_usr['user_id'].isin(no_tips_contacts['user_id'])\n",
        "H0: конверсия в просмотр контактов для двух сегментов одинаковая\n",
        "\n",
        "H1: конверсия значение просмотр контактов для двух сегментов разная\n",
        "\n",
        "# задаем уровень стат. значимости\n",
        "alpha = .05\n",
        "\n",
        "# проводим стат. тест о равенстве средних\n",
        "results = st.ttest_ind(tips_usr['is_success'],\n",
        "                       no_tips_usr['is_success'], equal_var=False)\n",
        "\n",
        "# выводим получившееся значение p-value и результаты стат. теста\n",
        "print('p-значение:', results.pvalue)\n",
        "\n",
        "if results.pvalue < alpha:\n",
        "    print('Отвергаем нулевую гипотезу')\n",
        "else:\n",
        "    print('Не получилось отвергнуть нулевую гипотезу')\n",
        "p-значение: 8.843583740520597e-12\n",
        "Отвергаем нулевую гипотезу\n",
        "P-value < 0.05, а значит мы отвергаем нулевую гипотезу и приримаем альтернативную (H1) - \"конверсия в просмотр контактов для двух сегментов разная\"\n",
        "\n",
        "Таким образом, мы можем заметить, что нет большой разницы между конверсией, расчитанной по сессимя и по юзерам. Для юзеров, использующих рекомендации, конверсия в просмотр контактов, расчитанная по уникам (юзерам), равна 30,64%, что больше на 8,87% чем расчитанная по сессиям, а вот для юзеров, игнорирующих рекомендации, составила 16,97% - больше лишь на 0,72%. Возможно, это связано с тем, что юзеры, кликающие на рекомендации, в среднем совершают больше действий за сессию, поэтому конверсия в контакт выше - ведь при прочих равных вероятность того, что они найдут нужный им товар - тоже выше. Далее мы это и проверим\n",
        "\n",
        "# считаем среднее значение количества действий в сессии для юзеров, использующих рекомендации\n",
        "mob_df[mob_df['user_id'].isin(tips_usr['user_id'])].groupby(['user_id', 'session_id'])['event_time'].count().mean()\n",
        "8.758571428571429\n",
        "# считаем медианное значение количества действий в сессии для юзеров, использующих рекомендации\n",
        "mob_df[mob_df['user_id'].isin(tips_usr['user_id'])].groupby(['user_id', 'session_id'])['event_time'].count().median()\n",
        "5.0\n",
        "# считаем среднее значение количества действий в сессии для юзеров, не использующих рекомендации\n",
        "mob_df[mob_df['user_id'].isin(no_tips_usr['user_id'])].groupby(['user_id', 'session_id'])['event_time'].count().mean()\n",
        "8.315719947159842\n",
        "# считаем медианное значение количества действий для юзеров, не использующих рекомендации\n",
        "mob_df[mob_df['user_id'].isin(no_tips_usr['user_id'])].groupby(['user_id', 'session_id'])['event_time'].count().median()\n",
        "5.0\n",
        "Юзеры, кликающие на рекомендации, в среднем совершают столько же действий, что и те, кто их игнорирует. А медианное значение равно для этих 2 сегментов. Можно сказать, что дело явно не в этом, возможно, стоит смотреть данные не внутри сессий, а в среднем по пользователям - далее мы посмотрим среднее и медианное кол-во действий на юзеров в разрезе данных сегментов.\n",
        "\n",
        "# считаем среднее значение количества действий для юзеров, использующих рекомендации\n",
        "mob_df[mob_df['user_id'].isin(tips_usr['user_id'])].groupby('user_id')['event_time'].count().mean()\n",
        "41.28619528619529\n",
        "# считаем медианное значение количества действий для юзеров, использующих рекомендации\n",
        "mob_df[mob_df['user_id'].isin(tips_usr['user_id'])].groupby('user_id')['event_time'].count().median()\n",
        "20.0\n",
        "# считаем среднее значение количества действий для юзеров, игнорирующих рекомендации\n",
        "mob_df[mob_df['user_id'].isin(no_tips_usr['user_id'])].groupby('user_id')['event_time'].count().mean()\n",
        "17.597843450479232\n",
        "# считаем медианное значение количества действий для юзеров, игнорирующих рекомендации\n",
        "mob_df[mob_df['user_id'].isin(no_tips_usr['user_id'])].groupby('user_id')['event_time'].count().median()\n",
        "10.0\n",
        "По этим данным различия уже есть. Так метрики юзеров, использующих рекомендации, почти в 2 раза выше. Среднее значением больше более чем в раза - 39 действий против 17.6, а медианное - 18 против 10. Таким образом, можно предположить, что большая конверсия в целевое действие у сегмента, использующего рекомендации, по причине того, что они совершают больше действий, что в свою очередь повышает вероятность того, что они свяжутся с продавцом - это мы и видим в данных. Однако момент в том, что внутри сессии активность юзеров в среднем одинаковая, дело в количестве сессий? Далее мы это и проверим\n",
        "\n",
        "# считаем среднее значение количества сессий для юзеров, использующих рекомендации\n",
        "(mob_df[mob_df['user_id'].isin(tips_usr[\"user_id\"])]\n",
        "       .groupby('user_id')\n",
        "       ['session_id'].nunique().mean())\n",
        "4.713804713804714\n",
        "# считаем среднее значение количества сессий для юзеров, игнорирующих рекомендации\n",
        "(mob_df[mob_df['user_id'].isin(no_tips_usr[\"user_id\"])]\n",
        "       .groupby('user_id')\n",
        "       ['session_id'].nunique().mean())\n",
        "2.116214057507987\n",
        "Подводя итог всему выше сказанному, можно выделить следующие моменты:\n",
        "\n",
        "конверсия в просмотр контактов для юзеров, просматривающих рекомендации (совершают действия 'tips_show' и 'tips_click') и игнорирующих их (совершают только 'tips_show'), разная;\n",
        "в среднем внутри сессии активность юзеров из данных сегментов одинаковая;\n",
        "а вот среднее количество сессий на юзера, использующего рекомендации, превышает количетсво сессий юзеров, игнорирующего их, более чем в 2 раза.\n",
        "Далее мы посмотрим на то, какие действий совершают юзеры из этих 2-ух сегментов.\n",
        "\n",
        "Расчет времени между событиями advert_open -> contacts_show и tips_click -> contacts_show\n",
        "# обновим фнукцию, написанную выше, для создания дф\n",
        "# для анализа прошедшего времени между \"tips_click\" и \"contacts_show\"\n",
        "def add_features2(df):\n",
        "    sorted_df = df.sort_values(by=['session_id', 'event_time'])\n",
        "    sorted_df['step'] = sorted_df.groupby('session_id').cumcount() + 1\n",
        "    sorted_df['source'] = sorted_df['event_name']\n",
        "    sorted_df['target'] = sorted_df.groupby('user_id')['source'].shift(-1)\n",
        "\n",
        "    return sorted_df\n",
        "# создадим копию нашего дф с информацией по сессиям и путем каждой сессии\n",
        "rec = add_features2(mob_df)\n",
        "# оставим те сессии юзеров, использующих рекомендации, в которых они просмотрели контакты\n",
        "rec = rec[rec['session_id'].isin(tips_contacts_session)]\n",
        "# оставим те строчки, где события \"tips_click\" или \"contacts_show\" - между которыми мы считаем время\n",
        "rec = rec[rec['event_name'].isin(['tips_click', 'contacts_show'])]\n",
        "# удалим дубли событий, оставив первые по времени появления в логах, внутри каждой сессии\n",
        "rec = rec.drop_duplicates(subset=['session_id', 'source'], keep='first')\n",
        "# осортируем дф\n",
        "rec.sort_values(['session_id', 'event_time'], inplace=True)\n",
        "# добавляем столбец со времени совершения первого нужного нам действия - \"tips_click\"\n",
        "rec['base_event_time'] = rec[rec['event_name'] == 'tips_click']['event_time']\n",
        "# заполним пропуски в столбце \"base_event_time\" в строках, где у нас действие не \"tips_click\"\n",
        "rec['base_event_time'] = rec.groupby('session_id')['base_event_time'].fillna(method='ffill').fillna(method='bfill')\n",
        "# заполним пропуски в столбце \"base_event_time\" в строках, где у нас действие не \"tips_click\"\n",
        "rec = rec[rec['event_time'] > rec['base_event_time']]\n",
        "# считаем разницу во времени между событиями \"tips_click\" и \"contacts_show\"\n",
        "rec['time_diff'] = rec['event_time'] - rec['base_event_time']\n",
        "# переформируем порядок и количество столбцов в дф\n",
        "rec = rec.iloc[:, [1, 0, 8, 9, 4]]\n",
        "# выводим среднее значение прошедшего времени между событиями \"tips_click\" и \"contacts_show\"\n",
        "rec['time_diff'].mean()\n",
        "Timedelta('0 days 00:12:41.285714285')\n",
        "Таким образом, мы получили, что после клика на рекомендацию проходит в среднем 12 минут 42 секунд прежде, чем юзеры просмотрят контакты. Не так уж мало времени, возможно, все это время пользователи рассматривают фото или же у нас есть случаи, крайне быстрого и медленного перехода от просмотра объявления к просмотру контактов, давайте посмотрем больше стат. инфы по столбцу с разницей времени между событиями.\n",
        "\n",
        "# выводим стат. информацию по столбцу со временем между нашими событиями\n",
        "rec['time_diff'].describe()\n",
        "count                           28\n",
        "mean     0 days 00:12:41.285714285\n",
        "std      0 days 00:12:14.725237488\n",
        "min                0 days 00:00:18\n",
        "25%                0 days 00:03:55\n",
        "50%                0 days 00:07:51\n",
        "75%         0 days 00:18:50.750000\n",
        "max                0 days 00:50:48\n",
        "Name: time_diff, dtype: object\n",
        "В нашем случае более корректно смотреть на медианное значение, ведь у нас есть четкое понимание того, что распределение наших наблюдение имеет положительную ассиметрию - правосторонний скос. Следовательно, мы знаем, что среднее значение искусственно завышает исследуемый показатель.\n",
        "\n",
        "Таким образом, возьмем 7 минут 51 секунд за бенчмарк прошедшего времени от клика на рекомендацию до просмотра контактов.\n",
        "\n",
        "# выделяем сессии юзеров, в которых было действие \"advert_open\"\n",
        "advert_session = mob_df.query('event_name == \"advert_open\"')['session_id'].unique()\n",
        "\n",
        "# выделяем сессии, в которых было действие \"advert_open\" и юзеры посмотрели контакты\n",
        "advert_contacts_session = set(advert_session) & set(contacts_session)\n",
        "# создадим копию нашего дф с информацией по сессиям и путем каждой сессии\n",
        "advert = add_features2(mob_df)\n",
        "# оставим те сессии юзеров, использующих рекомендации, в которых они просмотрели контакты\n",
        "advert = advert[advert['session_id'].isin(advert_contacts_session)]\n",
        "# оставим те строчки, где события \"advert_open\" или \"contacts_show\" - между которыми мы считаем время\n",
        "advert = advert[advert['event_name'].isin(['advert_open', 'contacts_show'])]\n",
        "# удалим дубли событий, оставив первые по времени появления в логах, внутри каждой сессии\n",
        "advert = advert.drop_duplicates(subset=['session_id', 'source'], keep='first')\n",
        "# осортируем дф\n",
        "advert.sort_values(['session_id', 'event_time'], inplace=True)\n",
        "# добавляем столбец со времени совершения первого нужного нам действия - \"advert_open\"\n",
        "advert['base_event_time'] = advert[advert['event_name'] == 'advert_open']['event_time']\n",
        "# заполним пропуски в столбце \"base_event_time\" в строках, где у нас действие не \"advert_open\"\n",
        "advert['base_event_time'] = advert.groupby('session_id')['base_event_time'].fillna(method='ffill').fillna(method='bfill')\n",
        "# заполним пропуски в столбце \"base_event_time\" в строках, где у нас действие не \"advert_open\"\n",
        "advert = advert[advert['event_time'] > advert['base_event_time']]\n",
        "# считаем разницу во времени между событиями \"advert_open\" и \"contacts_show\"\n",
        "advert['time_diff'] = advert['event_time'] - advert['base_event_time']\n",
        "# переформируем порядок и количество столбцов в дф\n",
        "advert = advert.iloc[:, [1, 0, 8, 9, 4]]\n",
        "# выводим среднее значение прошедшего времени между событиями \"advert_open\" и \"contacts_show\"\n",
        "advert['time_diff'].mean()\n",
        "Timedelta('0 days 00:07:24.151515151')\n",
        "В среднем от открытия карточки объявления до просмотра контактов проходит 7 минут 24 секнуды - меньше почти на 5,5 минут по сравнению с кликом на рекомендацию. Далее посмотрим больше стат. инфы по столбцу с разницей времени между событиями.\n",
        "\n",
        "# выводим стат. информацию по столбцу со временем между нашими событиями\n",
        "advert['time_diff'].describe()\n",
        "count                           99\n",
        "mean     0 days 00:07:24.151515151\n",
        "std      0 days 00:10:30.667485136\n",
        "min                0 days 00:00:03\n",
        "25%         0 days 00:00:59.500000\n",
        "50%                0 days 00:02:58\n",
        "75%         0 days 00:09:35.500000\n",
        "max                0 days 01:04:30\n",
        "Name: time_diff, dtype: object\n",
        "Несмотря на то, что наблюдений у нас гораздо больше - почти в 4 раза - ситуация аналогичная - медианное значение меньше среднего, в данном случае сильно меньше, а значит у нашего распределения правостороння ассиметрия.\n",
        "\n",
        "Если сравнивать значения по нашим рекомендациям и поисковой активности юзеров, то можно с уверенностью утверждать, что когда пользователи имеют конкретный запрос, то они быстрее переходят к просмтору контактов - оно и неудивительно. Ведь, кликая по рекомендации, юзер, возможно, даже и не задумывался о покупке данной вещи, поэтому ему требуется больше времени на то, чтобы перейти к просмотру контактов.\n",
        "\n",
        "Кажется, что разница в 5,5 минут между среднем пройденным времени по поисковой активности и рекомендациям вполне приемлемое значение, тоже что и по медианному - разница 5 минут.\n",
        "\n",
        "Таким образом, если судить по данной метрике - прошедшее время от просмотра объявления до просмотра контактов - то можно сказать, что наши рекомендации довольно точны и соответсвуют интересам наших юзеров, далее мы проверим еще и конверсию в целевое действие по данным событиям.\n",
        "\n",
        "# выделяем айди юзеров, использующих реки\n",
        "tips_usr = (mob_df[mob_df['event_name'] == 'tips_click']\n",
        "                                .groupby('user_id', as_index=False)\n",
        "                                .agg(base_event_time=('event_time', 'min')))\n",
        "\n",
        "# выделяем айди юзеров, которые посмотрели контакты и мерджим с предыдущим датафреймом\n",
        "tips_contacts_usr = (mob_df[mob_df['event_name'] == 'contacts_show']\n",
        "                                .merge(tips_usr, on='user_id', how='left'))\n",
        "\n",
        "# фильтруем, чтобы остались только те целевые события, что были после \"tips_click\"\n",
        "tips_contacts_usr = tips_contacts_usr[tips_contacts_usr['event_time'] > tips_contacts_usr['base_event_time']]\n",
        " # считаем CR из \"tips_click\" в целевое действие - \"contacts_show\"\n",
        "tips_contacts_usr['user_id'].nunique() / tips_usr['user_id'].nunique()\n",
        "0.2267080745341615\n",
        "# выделяем айди юзеров, которые совершали \"advert_open\"\n",
        "advert_usr = (mob_df[mob_df['event_name'] == 'advert_open']\n",
        "                                .groupby('user_id', as_index=False)\n",
        "                                .agg(base_event_time=('event_time', 'min')))\n",
        "\n",
        "# выделяем айди юзеров, которые посмотрели контакты и мерджим с предыдущим датафреймом\n",
        "advert_contacts_usr = (mob_df[mob_df['event_name'] == 'contacts_show']\n",
        "                                .merge(advert_usr, on='user_id', how='left'))\n",
        "\n",
        "# фильтруем, чтобы остались только те целевые события, что были после \"tips_click\"\n",
        "advert_contacts_usr = advert_contacts_usr[advert_contacts_usr['event_time'] > advert_contacts_usr['base_event_time']]\n",
        " # считаем CR из \"advert_open\" в целевое действие - \"contacts_show\"\n",
        "advert_contacts_usr['user_id'].nunique() / advert_usr['user_id'].nunique()\n",
        "0.15312916111850866\n",
        "По конверсии хорошо видно, что наши юзеры гораздо чаще переходят к просмотру контактов от наших рекомендаций - 22,67%, по сравнению с тем, когда они просматривают объявления, которые нашли сами - 15,31%. А значит нашим алгоритмы внутри рекомендательной системы предлагают именно то, что интересно юзерам, однако не стоит забывать, что в среднем от клика по рекомендации до просмотра контактов проходит на 5,5 минут больше, чем просто при переходе на карточку объявления.\n",
        "\n",
        "Данный факт можно объяснить тем, что юзерам может предлагаться товар из категории, о которой он даже не думал, но является \"комплементарной\" (связанной с категориями, из которых он смотрит товары), что и делает рекомендации более высококонверсионным каналом, но с большим временем между запросами контактов, ведь юзеру нужно изучить информацию о новой для него категории товаров.\n",
        "\n",
        "Проверка гипотезы: \"пользователи одинаково конвертятся в просмотр контактов из поиска и рекомендаций\"\n",
        "# конверсия из рекомендаций в просмотор контактов\n",
        "tips_cr = tips_contacts_usr['user_id'].nunique() / len(tips_usr)\n",
        "tips_cr\n",
        "0.2267080745341615\n",
        "# выделяем айди юзеров, использующих реки\n",
        "search_usr = (mob_df[mob_df['event_name'] == 'search']\n",
        "                                .groupby('user_id', as_index=False)\n",
        "                                .agg(base_event_time=('event_time', 'min')))\n",
        "\n",
        "# выделяем айди юзеров, которые посмотрели контакты и мерджим с предыдущим датафреймом\n",
        "search_contacts_usr = (mob_df[mob_df['event_name'] == 'contacts_show']\n",
        "                                .merge(search_usr, on='user_id', how='left'))\n",
        "\n",
        "# фильтруем, чтобы остались только те целевые события, что были после tips_show\n",
        "search_contacts_usr = search_contacts_usr[search_contacts_usr['event_time'] > search_contacts_usr['base_event_time']]\n",
        "# конверсия из поиска в просмотор контактов\n",
        "serach_cr = search_contacts_usr['user_id'].nunique() / len(search_usr)\n",
        "serach_cr\n",
        "0.20528211284513806\n",
        "Как можно заметить, конверсия в просмотр контактов разная, однако без проведения стат. теста мы не можем с уверенностью утверждать, что мы имеем стат. значимое различие, так что переходим к проведению стат. теста\n",
        "\n",
        "H0: конверсия в просмотр контактов из поиска и рекомендаций одинаковая;\n",
        "H1: конверсия в просмотр контактов из поиска и рекомендаций разная;\n",
        "alpha = 0.05  # критический уровень статистической значимости\n",
        "\n",
        "cr_combined = ((tips_contacts_usr['user_id'].nunique() + search_contacts_usr['user_id'].nunique())\n",
        "                / (len(tips_usr) + len(search_usr)))\n",
        "\n",
        "difference =  tips_cr - serach_cr\n",
        "\n",
        "z_value = difference / mth.sqrt(cr_combined * (1 - cr_combined) * (1/len(tips_usr) + 1/len(search_usr)))\n",
        "\n",
        "distr = st.norm(0, 1)\n",
        "\n",
        "p_value = (1 - distr.cdf(abs(z_value))) * 2\n",
        "\n",
        "print('p-значение: ', p_value)\n",
        "\n",
        "if p_value < alpha:\n",
        "    print('Отвергаем нулевую гипотезу: между конверсиями есть значимая разница')\n",
        "else:\n",
        "    print(\n",
        "        'Не получилось отвергнуть нулевую гипотезу, нет оснований считать конверсии разными'\n",
        "    )\n",
        "p-значение:  0.3864817999613277\n",
        "Не получилось отвергнуть нулевую гипотезу, нет оснований считать конверсии разными\n",
        "P-value > 0.05, а значит мы принимаем нулевую гипотезу - \"конверсия в просмотр контактов из поиска и рекомендаций одинаковая\"\n",
        "\n",
        "Несмотря на то, что конверсия просмотров контактов из поиска и из рекомендаций разная - 20,53% и 22,67% соответственно, данные различия в данном случае не являются стат. значимыми.\n",
        "\n",
        "Однако факт того, что конверсия в целевое действие - просмотр контактов - из поисковой активности меньше, чем из рекомендаций свидетельствует о том, что наши алгоритмы хоть и уступают в некоторых ситуациях - когда у пользователя есть конкретный запрос - но в целом, предложенные нами объявления достаточно точно попадают в интересы наших юзеров.\n",
        "\n",
        "Шаг 5. Итоги исследования\n",
        "в среднем наши юзеры совершают по 17 действий, медианное же значение - 9 действий;\n",
        "для юзеров, использующих рекомендации, доля \"tips_show\" в общем кол-ве событий больше на 20%, 71,4% против 50,4%;\n",
        "топ 5 событий юзеров, использующих рекомендации, выглядит следующим образом (название события - доля в общем кол-ве логов):\n",
        "показ рекомендаций - 71,4%;\n",
        "просмотр контактов - 6,9%;\n",
        "клик на рекомендацию - 6,5%;\n",
        "просмотр карты объявлений - 5,3%;\n",
        "открытие карточки объявления - 5,28%.\n",
        "а для юзеров, игнорирующих рекомендации:\n",
        "показ рекомендаций - 50,4%;\n",
        "просмотр фото - 16,2%;\n",
        "поисковая активность - 10,4%;\n",
        "открытие карточки объявления - 8,9%;\n",
        "просмотр контактов - 5,9%.\n",
        "доля пользователей, просмотревших контакты, больше всего в источнике – \"yandex\" - 24,7%, чуть меньше в \"google\" c 24,4%, а наименьшая доля в \"other\" - лишь 18,5%, если мы хотим растить долю пользователей, запрашивающих контакты, то в первую очередь стоит смотреть на масштабирование каналов \"yandex\" или \"google\", при условии, что стоимость привлечения одинаковая для всех каналов;\n",
        "конверсия в \"contacts_show\" выше при переходе от рекомендаций \"tips_click\" - 22,67%, по сравнению с переходом от \"advert_open\" - 15,31%\n",
        "наиболее популярным двухэтапным сценарием использования приложения является \"tips_show\" -> \"contacts_show\" - конверсия в целевое действие составила 17,7%;\n",
        "наибольшая конверсия в целевое действие среди двухэтапных сценариев у пути от \"photos_show\" к \"contacts_show\" - 27,5%;\n",
        "среди трехэтапных сессий наибольшая конверсия в целевое действие у сценария \"map\" -> \"tips_show\" -> \"contacts_show\" - 18,9%;\n",
        "чем меньше шагов до целевого события, тем выше конверсия в его совершение.\n",
        "Рекомендации\n",
        "Как мне кажется, проведение пользовательских интервью для каждого сегмента юзеров - тех, кто обычно приходит в приложение с конкретным запросом (сессии начинаются с \"search\"); тех, кто готов полистать наши рекомендации (сессии начинаются с \"tips_show\" или содержат много данных логов внутри сессии) и для остальных сегментов, рассмотренных в исследовании позволит спроектировать сценарии использования с учетом особенностей каждого сегмента, что в конечном итоге может приветси к сокращению средней длины пути до целевого действия. А как было сказано выше, что чем меньше шагов до целевого действия, тем выше конверсия;\n",
        "сосредоточиться на развитии рекомендательной системы, поскольку юзеры, использующие реки, значительно больше времени проводят в приложении (в среднем 4,5 сессии против 2,2), а также проявляют большую вовлеченность;\n",
        "поскольку просмотр контактов является важной метрикой для нашего сервиса, то крайне важно сделать эту функцию более заметной, удобной и интуитивно понятной для юзеров, здесь опять же помогут пользовательские интервью;\n",
        "при масштабировании каналов рекламы смотреть прежде всего в сторону \"Яндекс\" и \"Google\" - каналов с высокой конверсией в целевое действие - просмотров контактов;\n",
        "используйте опыт успешных (по конверсии) двухэтапных и трехэтапных сценариев: по данным сценарии:\n",
        "\"tips_show\" -> \"contacts_show\" имеет конверсию 17,7%;\n",
        "\"photos_show\" -> \"contacts_show\" самый высоко конверсионный сценарий - 27,5% - среди двухэтапных сценариев.\n",
        "оптимизируйте и продвигайте эти сценарии, чтобы большее число пользователей выполнить целевое действие, аналогично с трехэтапными сценариями, \"map\" -> \"tips_show\" -> \"contacts_show\" имеет самый высокий коэффициент конверсии - 18,9%. На примере данного трехэтапного путя хорошо видно, насколько влияют качественные рекомендации на конверсию в целевое действие;\n",
        "регулярно отслеживать основные метрики по выделенным сегментам пользователей;\n",
        "провести A/B тест, при попытке масштабирования одного из высоко конверсионного сценария, чтобы не обрушить конверсию по другим сценариям."
      ]
    }
  ]
}